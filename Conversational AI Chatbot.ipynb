{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align='center'>Conversational AI Chatbot</h1>"
      ],
      "metadata": {
        "id": "rYMXTM-Ks--j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A chatbot is a software or computer program that simulates human conversation or \"chatter\" through text or voice interactions.\n",
        "\n",
        "Chatbot are used by almost every tech based company and become trending these days"
      ],
      "metadata": {
        "id": "paZPgO4ctLYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkUzz_SQtMQs",
        "outputId": "b2b24083-3795-467d-bc7e-d06c272f2af6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "ZKnhzhRwuBuv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers , activations , models , preprocessing , utils\n",
        "import pandas as pd\n",
        "import pickle\n",
        "print( tf.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF63TvIqtS4D",
        "outputId": "fb8bc2ff-d6c4-428f-81cb-81e55b67ef1d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the data"
      ],
      "metadata": {
        "id": "UATBeb3quFEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path='chatbot.txt'"
      ],
      "metadata": {
        "id": "Pg2ZHin7tu4K"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "since data path is a directory to access it i  have used the following lines to extract it"
      ],
      "metadata": {
        "id": "wL2gv3tUuJrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = []\n",
        "target_texts = []\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "for line in lines[: min(600, len(lines) - 1)]:\n",
        "    input_text = line.split('\\t')[0]\n",
        "    target_text = line.split('\\t')[1]\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)"
      ],
      "metadata": {
        "id": "uLZnaQYKtxwe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zippedList =  list(zip(input_texts, target_texts))\n",
        "lines = pd.DataFrame(zippedList, columns = ['input' , 'output']) "
      ],
      "metadata": {
        "id": "bwA2vzZft0EI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zXPh83iFuVLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "since the chatbot is train over seq2seq model as the data is sequential\n",
        "for seq2seq encoder-decoder will best choice to train chatbot"
      ],
      "metadata": {
        "id": "ysLAwL3buP6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing input data for the Encoder"
      ],
      "metadata": {
        "id": "JfZOsjseuWSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_lines = list()\n",
        "for line in lines.input:\n",
        "    input_lines.append( line ) \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( input_lines ) \n",
        "tokenized_input_lines = tokenizer.texts_to_sequences( input_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_input_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_input_length = np.array( length_list ).max()\n",
        "print( 'Input max length is {}'.format( max_input_length ))\n",
        "\n",
        "padded_input_lines = preprocessing.sequence.pad_sequences( tokenized_input_lines , maxlen=max_input_length , padding='post' )\n",
        "encoder_input_data = np.array( padded_input_lines )\n",
        "print( 'Encoder input data shape -> {}'.format( encoder_input_data.shape ))\n",
        "\n",
        "input_word_dict = tokenizer.word_index\n",
        "num_input_tokens = len( input_word_dict )+1\n",
        "print( 'Number of Input tokens = {}'.format( num_input_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH57Ze2tt1pd",
        "outputId": "fd0632e9-9928-43eb-c544-ac129870d58a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input max length is 22\n",
            "Encoder input data shape -> (566, 22)\n",
            "Number of Input tokens = 518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H36dVb-8t3A8",
        "outputId": "02088351-2fa2-485d-b804-e6db9e58a4a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  4,   3,  12, ...,   0,   0,   0],\n",
              "       [  4,   3,  12, ...,   0,   0,   0],\n",
              "       [  4,   3,  12, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [498, 499,   9, ...,   0,   0,   0],\n",
              "       [  9, 207, 208, ...,   0,   0,   0],\n",
              "       [510, 192,   5, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing input data for the Decoder\n"
      ],
      "metadata": {
        "id": "4c4qRVvkuftP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_lines = list()\n",
        "for line in lines.output:\n",
        "    output_lines.append( '<START> ' + line + ' <END>' )  \n",
        "\n",
        "tokenizer = preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts( output_lines ) \n",
        "tokenized_output_lines = tokenizer.texts_to_sequences( output_lines ) \n",
        "\n",
        "length_list = list()\n",
        "for token_seq in tokenized_output_lines:\n",
        "    length_list.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list ).max()\n",
        "print( 'Output max length is {}'.format( max_output_length ))\n",
        "\n",
        "padded_output_lines = preprocessing.sequence.pad_sequences( tokenized_output_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_output_lines )\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
        "\n",
        "output_word_dict = tokenizer.word_index\n",
        "num_output_tokens = len( output_word_dict )+1\n",
        "print( 'Number of Output tokens = {}'.format( num_output_tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L0Frq11ucRs",
        "outputId": "08111419-3fdd-43ba-bd5b-dcf3056a0d7e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output max length is 74\n",
            "Decoder input data shape -> (566, 74)\n",
            "Number of Output tokens = 1692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing target data for the Decoder "
      ],
      "metadata": {
        "id": "vApNQbjmumbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target_data = list()\n",
        "for token_seq in tokenized_output_lines:\n",
        "    decoder_target_data.append( token_seq[ 1 : ] ) \n",
        "    \n",
        "padded_output_lines = preprocessing.sequence.pad_sequences( decoder_target_data , maxlen=max_output_length, padding='post' )\n",
        "onehot_output_lines = utils.to_categorical( padded_output_lines , num_output_tokens )\n",
        "decoder_target_data = np.array(onehot_output_lines )\n",
        "print( 'Decoder target data shape -> {}'.format( decoder_target_data.shape ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slPD0xbyuj76",
        "outputId": "b0ba24e2-1bf2-4bce-fb23-7c59832545f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder target data shape -> (566, 74, 1692)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Model\n"
      ],
      "metadata": {
        "id": "4Vygy5mfussc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=( None , ))\n",
        "encoder_embedding = tf.keras.layers.Embedding( num_input_tokens, 256 , mask_zero=True ) (encoder_inputs)\n",
        "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 256 , return_state=True , recurrent_dropout=0.2 , dropout=0.2 )( encoder_embedding )\n",
        "encoder_states = [ state_h , state_c ]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=( None ,  ))\n",
        "decoder_embedding = tf.keras.layers.Embedding( num_output_tokens, 256 , mask_zero=True) (decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM( 256 , return_state=True , return_sequences=True , recurrent_dropout=0.2 , dropout=0.2)\n",
        "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
        "decoder_dense = tf.keras.layers.Dense( num_output_tokens , activation=tf.keras.activations.softmax ) \n",
        "output = decoder_dense ( decoder_outputs )\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxTbCfozupeK",
        "outputId": "9b4699f3-fbf9-4749-b122-e9074f36de79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, None, 256)    132608      ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 256)    433152      ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 256),        525312      ['embedding[0][0]']              \n",
            "                                 (None, 256),                                                     \n",
            "                                 (None, 256)]                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, None, 256),  525312      ['embedding_1[0][0]',            \n",
            "                                 (None, 256),                     'lstm[0][1]',                   \n",
            "                                 (None, 256)]                     'lstm[0][2]']                   \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, None, 1692)   434844      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,051,228\n",
            "Trainable params: 2,051,228\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit([encoder_input_data , decoder_input_data], decoder_target_data, batch_size=124, epochs=500) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISrFx4ORuvo-",
        "outputId": "f17ac8b9-08c9-4aa5-afc3-3dbc95f212cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "5/5 [==============================] - 10s 519ms/step - loss: 1.4405\n",
            "Epoch 2/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 1.4319\n",
            "Epoch 3/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 1.3447\n",
            "Epoch 4/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 1.1863\n",
            "Epoch 5/500\n",
            "5/5 [==============================] - 3s 529ms/step - loss: 1.1423\n",
            "Epoch 6/500\n",
            "5/5 [==============================] - 3s 528ms/step - loss: 1.1218\n",
            "Epoch 7/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 1.1114\n",
            "Epoch 8/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 1.1046\n",
            "Epoch 9/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 1.0993\n",
            "Epoch 10/500\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 1.0936\n",
            "Epoch 11/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 1.0878\n",
            "Epoch 12/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 1.0813\n",
            "Epoch 13/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 1.0732\n",
            "Epoch 14/500\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 1.0651\n",
            "Epoch 15/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 1.0566\n",
            "Epoch 16/500\n",
            "5/5 [==============================] - 3s 495ms/step - loss: 1.0485\n",
            "Epoch 17/500\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 1.0411\n",
            "Epoch 18/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 1.0348\n",
            "Epoch 19/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 1.0282\n",
            "Epoch 20/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 1.0228\n",
            "Epoch 21/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 1.0157\n",
            "Epoch 22/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 1.0093\n",
            "Epoch 23/500\n",
            "5/5 [==============================] - 3s 528ms/step - loss: 1.0025\n",
            "Epoch 24/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.9958\n",
            "Epoch 25/500\n",
            "5/5 [==============================] - 3s 499ms/step - loss: 0.9889\n",
            "Epoch 26/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.9816\n",
            "Epoch 27/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.9743\n",
            "Epoch 28/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.9668\n",
            "Epoch 29/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.9598\n",
            "Epoch 30/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.9518\n",
            "Epoch 31/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.9442\n",
            "Epoch 32/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.9364\n",
            "Epoch 33/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.9286\n",
            "Epoch 34/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.9205\n",
            "Epoch 35/500\n",
            "5/5 [==============================] - 3s 552ms/step - loss: 0.9129\n",
            "Epoch 36/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.9050\n",
            "Epoch 37/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.8974\n",
            "Epoch 38/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.8901\n",
            "Epoch 39/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.8829\n",
            "Epoch 40/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.8753\n",
            "Epoch 41/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.8681\n",
            "Epoch 42/500\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.8610\n",
            "Epoch 43/500\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.8541\n",
            "Epoch 44/500\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.8475\n",
            "Epoch 45/500\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.8405\n",
            "Epoch 46/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.8340\n",
            "Epoch 47/500\n",
            "5/5 [==============================] - 3s 529ms/step - loss: 0.8277\n",
            "Epoch 48/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.8214\n",
            "Epoch 49/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.8148\n",
            "Epoch 50/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.8089\n",
            "Epoch 51/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.8026\n",
            "Epoch 52/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.7963\n",
            "Epoch 53/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.7905\n",
            "Epoch 54/500\n",
            "5/5 [==============================] - 3s 524ms/step - loss: 0.7842\n",
            "Epoch 55/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.7785\n",
            "Epoch 56/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.7727\n",
            "Epoch 57/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.7667\n",
            "Epoch 58/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.7607\n",
            "Epoch 59/500\n",
            "5/5 [==============================] - 3s 549ms/step - loss: 0.7551\n",
            "Epoch 60/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.7487\n",
            "Epoch 61/500\n",
            "5/5 [==============================] - 3s 498ms/step - loss: 0.7436\n",
            "Epoch 62/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.7372\n",
            "Epoch 63/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.7319\n",
            "Epoch 64/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.7262\n",
            "Epoch 65/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.7202\n",
            "Epoch 66/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.7148\n",
            "Epoch 67/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.7090\n",
            "Epoch 68/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.7033\n",
            "Epoch 69/500\n",
            "5/5 [==============================] - 3s 528ms/step - loss: 0.6978\n",
            "Epoch 70/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.6924\n",
            "Epoch 71/500\n",
            "5/5 [==============================] - 3s 529ms/step - loss: 0.6868\n",
            "Epoch 72/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.6818\n",
            "Epoch 73/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.6758\n",
            "Epoch 74/500\n",
            "5/5 [==============================] - 3s 532ms/step - loss: 0.6704\n",
            "Epoch 75/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.6651\n",
            "Epoch 76/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.6595\n",
            "Epoch 77/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.6545\n",
            "Epoch 78/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.6492\n",
            "Epoch 79/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.6433\n",
            "Epoch 80/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.6387\n",
            "Epoch 81/500\n",
            "5/5 [==============================] - 3s 493ms/step - loss: 0.6332\n",
            "Epoch 82/500\n",
            "5/5 [==============================] - 3s 528ms/step - loss: 0.6275\n",
            "Epoch 83/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.6230\n",
            "Epoch 84/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.6176\n",
            "Epoch 85/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.6126\n",
            "Epoch 86/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.6068\n",
            "Epoch 87/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.6017\n",
            "Epoch 88/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.5967\n",
            "Epoch 89/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.5914\n",
            "Epoch 90/500\n",
            "5/5 [==============================] - 3s 498ms/step - loss: 0.5862\n",
            "Epoch 91/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.5814\n",
            "Epoch 92/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.5762\n",
            "Epoch 93/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.5714\n",
            "Epoch 94/500\n",
            "5/5 [==============================] - 3s 564ms/step - loss: 0.5651\n",
            "Epoch 95/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.5607\n",
            "Epoch 96/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.5556\n",
            "Epoch 97/500\n",
            "5/5 [==============================] - 3s 529ms/step - loss: 0.5505\n",
            "Epoch 98/500\n",
            "5/5 [==============================] - 3s 523ms/step - loss: 0.5451\n",
            "Epoch 99/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.5409\n",
            "Epoch 100/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.5351\n",
            "Epoch 101/500\n",
            "5/5 [==============================] - 3s 523ms/step - loss: 0.5304\n",
            "Epoch 102/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.5248\n",
            "Epoch 103/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.5198\n",
            "Epoch 104/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.5149\n",
            "Epoch 105/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.5105\n",
            "Epoch 106/500\n",
            "5/5 [==============================] - 3s 536ms/step - loss: 0.5056\n",
            "Epoch 107/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.5006\n",
            "Epoch 108/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.4958\n",
            "Epoch 109/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.4913\n",
            "Epoch 110/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.4857\n",
            "Epoch 111/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.4814\n",
            "Epoch 112/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.4760\n",
            "Epoch 113/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.4711\n",
            "Epoch 114/500\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.4669\n",
            "Epoch 115/500\n",
            "5/5 [==============================] - 3s 525ms/step - loss: 0.4611\n",
            "Epoch 116/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.4567\n",
            "Epoch 117/500\n",
            "5/5 [==============================] - 3s 530ms/step - loss: 0.4524\n",
            "Epoch 118/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.4479\n",
            "Epoch 119/500\n",
            "5/5 [==============================] - 3s 531ms/step - loss: 0.4428\n",
            "Epoch 120/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.4377\n",
            "Epoch 121/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.4336\n",
            "Epoch 122/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.4289\n",
            "Epoch 123/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.4248\n",
            "Epoch 124/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.4206\n",
            "Epoch 125/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.4159\n",
            "Epoch 126/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.4114\n",
            "Epoch 127/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.4072\n",
            "Epoch 128/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.4029\n",
            "Epoch 129/500\n",
            "5/5 [==============================] - 3s 531ms/step - loss: 0.3982\n",
            "Epoch 130/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.3944\n",
            "Epoch 131/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.3901\n",
            "Epoch 132/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.3861\n",
            "Epoch 133/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.3819\n",
            "Epoch 134/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.3781\n",
            "Epoch 135/500\n",
            "5/5 [==============================] - 3s 499ms/step - loss: 0.3738\n",
            "Epoch 136/500\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.3699\n",
            "Epoch 137/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.3657\n",
            "Epoch 138/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.3616\n",
            "Epoch 139/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.3585\n",
            "Epoch 140/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.3543\n",
            "Epoch 141/500\n",
            "5/5 [==============================] - 3s 546ms/step - loss: 0.3503\n",
            "Epoch 142/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.3474\n",
            "Epoch 143/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.3433\n",
            "Epoch 144/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.3396\n",
            "Epoch 145/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.3358\n",
            "Epoch 146/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.3320\n",
            "Epoch 147/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.3287\n",
            "Epoch 148/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.3246\n",
            "Epoch 149/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.3212\n",
            "Epoch 150/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.3176\n",
            "Epoch 151/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.3144\n",
            "Epoch 152/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.3119\n",
            "Epoch 153/500\n",
            "5/5 [==============================] - 3s 525ms/step - loss: 0.3098\n",
            "Epoch 154/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.3044\n",
            "Epoch 155/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.3016\n",
            "Epoch 156/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.2985\n",
            "Epoch 157/500\n",
            "5/5 [==============================] - 3s 524ms/step - loss: 0.2957\n",
            "Epoch 158/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.2917\n",
            "Epoch 159/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.2885\n",
            "Epoch 160/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.2861\n",
            "Epoch 161/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.2828\n",
            "Epoch 162/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.2799\n",
            "Epoch 163/500\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.2763\n",
            "Epoch 164/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.2734\n",
            "Epoch 165/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.2708\n",
            "Epoch 166/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.2681\n",
            "Epoch 167/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.2657\n",
            "Epoch 168/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.2627\n",
            "Epoch 169/500\n",
            "5/5 [==============================] - 3s 525ms/step - loss: 0.2599\n",
            "Epoch 170/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.2574\n",
            "Epoch 171/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.2544\n",
            "Epoch 172/500\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 0.2521\n",
            "Epoch 173/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.2490\n",
            "Epoch 174/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.2458\n",
            "Epoch 175/500\n",
            "5/5 [==============================] - 3s 499ms/step - loss: 0.2437\n",
            "Epoch 176/500\n",
            "5/5 [==============================] - 3s 499ms/step - loss: 0.2412\n",
            "Epoch 177/500\n",
            "5/5 [==============================] - 3s 529ms/step - loss: 0.2385\n",
            "Epoch 178/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.2373\n",
            "Epoch 179/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.2342\n",
            "Epoch 180/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.2316\n",
            "Epoch 181/500\n",
            "5/5 [==============================] - 3s 499ms/step - loss: 0.2291\n",
            "Epoch 182/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.2265\n",
            "Epoch 183/500\n",
            "5/5 [==============================] - 3s 498ms/step - loss: 0.2246\n",
            "Epoch 184/500\n",
            "5/5 [==============================] - 3s 527ms/step - loss: 0.2227\n",
            "Epoch 185/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.2197\n",
            "Epoch 186/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.2180\n",
            "Epoch 187/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.2153\n",
            "Epoch 188/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.2130\n",
            "Epoch 189/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.2116\n",
            "Epoch 190/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.2086\n",
            "Epoch 191/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.2075\n",
            "Epoch 192/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.2050\n",
            "Epoch 193/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.2040\n",
            "Epoch 194/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.2015\n",
            "Epoch 195/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.1991\n",
            "Epoch 196/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.1970\n",
            "Epoch 197/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.1950\n",
            "Epoch 198/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.1934\n",
            "Epoch 199/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.1906\n",
            "Epoch 200/500\n",
            "5/5 [==============================] - 3s 524ms/step - loss: 0.1902\n",
            "Epoch 201/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.1878\n",
            "Epoch 202/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.1856\n",
            "Epoch 203/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.1846\n",
            "Epoch 204/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.1825\n",
            "Epoch 205/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.1804\n",
            "Epoch 206/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.1789\n",
            "Epoch 207/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.1774\n",
            "Epoch 208/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.1755\n",
            "Epoch 209/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.1736\n",
            "Epoch 210/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.1719\n",
            "Epoch 211/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.1696\n",
            "Epoch 212/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.1682\n",
            "Epoch 213/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.1677\n",
            "Epoch 214/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.1658\n",
            "Epoch 215/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.1641\n",
            "Epoch 216/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.1629\n",
            "Epoch 217/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.1608\n",
            "Epoch 218/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.1589\n",
            "Epoch 219/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.1578\n",
            "Epoch 220/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.1562\n",
            "Epoch 221/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.1543\n",
            "Epoch 222/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.1532\n",
            "Epoch 223/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.1522\n",
            "Epoch 224/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.1513\n",
            "Epoch 225/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.1495\n",
            "Epoch 226/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.1467\n",
            "Epoch 227/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.1461\n",
            "Epoch 228/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.1454\n",
            "Epoch 229/500\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.1435\n",
            "Epoch 230/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.1422\n",
            "Epoch 231/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.1415\n",
            "Epoch 232/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.1408\n",
            "Epoch 233/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.1384\n",
            "Epoch 234/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.1383\n",
            "Epoch 235/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.1360\n",
            "Epoch 236/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.1360\n",
            "Epoch 237/500\n",
            "5/5 [==============================] - 3s 532ms/step - loss: 0.1345\n",
            "Epoch 238/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.1323\n",
            "Epoch 239/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.1316\n",
            "Epoch 240/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.1308\n",
            "Epoch 241/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.1289\n",
            "Epoch 242/500\n",
            "5/5 [==============================] - 3s 529ms/step - loss: 0.1281\n",
            "Epoch 243/500\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.1265\n",
            "Epoch 244/500\n",
            "5/5 [==============================] - 2s 488ms/step - loss: 0.1250\n",
            "Epoch 245/500\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.1242\n",
            "Epoch 246/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.1227\n",
            "Epoch 247/500\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.1211\n",
            "Epoch 248/500\n",
            "5/5 [==============================] - 2s 491ms/step - loss: 0.1214\n",
            "Epoch 249/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.1199\n",
            "Epoch 250/500\n",
            "5/5 [==============================] - 2s 500ms/step - loss: 0.1189\n",
            "Epoch 251/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.1177\n",
            "Epoch 252/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.1169\n",
            "Epoch 253/500\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.1153\n",
            "Epoch 254/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.1143\n",
            "Epoch 255/500\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.1139\n",
            "Epoch 256/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.1127\n",
            "Epoch 257/500\n",
            "5/5 [==============================] - 3s 541ms/step - loss: 0.1119\n",
            "Epoch 258/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.1109\n",
            "Epoch 259/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.1087\n",
            "Epoch 260/500\n",
            "5/5 [==============================] - 3s 529ms/step - loss: 0.1098\n",
            "Epoch 261/500\n",
            "5/5 [==============================] - 3s 533ms/step - loss: 0.1076\n",
            "Epoch 262/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.1067\n",
            "Epoch 263/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.1065\n",
            "Epoch 264/500\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.1055\n",
            "Epoch 265/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.1045\n",
            "Epoch 266/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.1034\n",
            "Epoch 267/500\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.1022\n",
            "Epoch 268/500\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.1017\n",
            "Epoch 269/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.1010\n",
            "Epoch 270/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.1007\n",
            "Epoch 271/500\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.0999\n",
            "Epoch 272/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.0985\n",
            "Epoch 273/500\n",
            "5/5 [==============================] - 3s 523ms/step - loss: 0.0983\n",
            "Epoch 274/500\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.0963\n",
            "Epoch 275/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0963\n",
            "Epoch 276/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.0947\n",
            "Epoch 277/500\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.0943\n",
            "Epoch 278/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.0942\n",
            "Epoch 279/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.0930\n",
            "Epoch 280/500\n",
            "5/5 [==============================] - 3s 498ms/step - loss: 0.0929\n",
            "Epoch 281/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.0917\n",
            "Epoch 282/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.0903\n",
            "Epoch 283/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0893\n",
            "Epoch 284/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.0889\n",
            "Epoch 285/500\n",
            "5/5 [==============================] - 3s 553ms/step - loss: 0.0882\n",
            "Epoch 286/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.0871\n",
            "Epoch 287/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.0873\n",
            "Epoch 288/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0858\n",
            "Epoch 289/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.0850\n",
            "Epoch 290/500\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.0845\n",
            "Epoch 291/500\n",
            "5/5 [==============================] - 3s 531ms/step - loss: 0.0847\n",
            "Epoch 292/500\n",
            "5/5 [==============================] - 3s 499ms/step - loss: 0.0831\n",
            "Epoch 293/500\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.0822\n",
            "Epoch 294/500\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.0822\n",
            "Epoch 295/500\n",
            "5/5 [==============================] - 3s 499ms/step - loss: 0.0818\n",
            "Epoch 296/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.0804\n",
            "Epoch 297/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.0804\n",
            "Epoch 298/500\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.0788\n",
            "Epoch 299/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.0785\n",
            "Epoch 300/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.0774\n",
            "Epoch 301/500\n",
            "5/5 [==============================] - 3s 494ms/step - loss: 0.0775\n",
            "Epoch 302/500\n",
            "5/5 [==============================] - 3s 538ms/step - loss: 0.0774\n",
            "Epoch 303/500\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.0763\n",
            "Epoch 304/500\n",
            "5/5 [==============================] - 2s 486ms/step - loss: 0.0754\n",
            "Epoch 305/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.0753\n",
            "Epoch 306/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0748\n",
            "Epoch 307/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.0751\n",
            "Epoch 308/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0740\n",
            "Epoch 309/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.0730\n",
            "Epoch 310/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.0721\n",
            "Epoch 311/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.0715\n",
            "Epoch 312/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.0706\n",
            "Epoch 313/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.0707\n",
            "Epoch 314/500\n",
            "5/5 [==============================] - 2s 485ms/step - loss: 0.0703\n",
            "Epoch 315/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.0698\n",
            "Epoch 316/500\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.0686\n",
            "Epoch 317/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0680\n",
            "Epoch 318/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.0683\n",
            "Epoch 319/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.0673\n",
            "Epoch 320/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.0673\n",
            "Epoch 321/500\n",
            "5/5 [==============================] - 3s 495ms/step - loss: 0.0663\n",
            "Epoch 322/500\n",
            "5/5 [==============================] - 3s 531ms/step - loss: 0.0659\n",
            "Epoch 323/500\n",
            "5/5 [==============================] - 3s 497ms/step - loss: 0.0658\n",
            "Epoch 324/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.0657\n",
            "Epoch 325/500\n",
            "5/5 [==============================] - 2s 487ms/step - loss: 0.0646\n",
            "Epoch 326/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.0642\n",
            "Epoch 327/500\n",
            "5/5 [==============================] - 3s 498ms/step - loss: 0.0634\n",
            "Epoch 328/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0635\n",
            "Epoch 329/500\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.0630\n",
            "Epoch 330/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0629\n",
            "Epoch 331/500\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.0615\n",
            "Epoch 332/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.0616\n",
            "Epoch 333/500\n",
            "5/5 [==============================] - 3s 524ms/step - loss: 0.0607\n",
            "Epoch 334/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.0603\n",
            "Epoch 335/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.0603\n",
            "Epoch 336/500\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.0599\n",
            "Epoch 337/500\n",
            "5/5 [==============================] - 2s 499ms/step - loss: 0.0594\n",
            "Epoch 338/500\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.0594\n",
            "Epoch 339/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.0587\n",
            "Epoch 340/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.0580\n",
            "Epoch 341/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0584\n",
            "Epoch 342/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.0574\n",
            "Epoch 343/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0569\n",
            "Epoch 344/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.0564\n",
            "Epoch 345/500\n",
            "5/5 [==============================] - 3s 546ms/step - loss: 0.0564\n",
            "Epoch 346/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.0556\n",
            "Epoch 347/500\n",
            "5/5 [==============================] - 3s 535ms/step - loss: 0.0554\n",
            "Epoch 348/500\n",
            "5/5 [==============================] - 3s 495ms/step - loss: 0.0544\n",
            "Epoch 349/500\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.0547\n",
            "Epoch 350/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.0547\n",
            "Epoch 351/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.0535\n",
            "Epoch 352/500\n",
            "5/5 [==============================] - 3s 534ms/step - loss: 0.0532\n",
            "Epoch 353/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.0533\n",
            "Epoch 354/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.0528\n",
            "Epoch 355/500\n",
            "5/5 [==============================] - 3s 528ms/step - loss: 0.0525\n",
            "Epoch 356/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.0522\n",
            "Epoch 357/500\n",
            "5/5 [==============================] - 3s 525ms/step - loss: 0.0523\n",
            "Epoch 358/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.0513\n",
            "Epoch 359/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0512\n",
            "Epoch 360/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.0509\n",
            "Epoch 361/500\n",
            "5/5 [==============================] - 3s 528ms/step - loss: 0.0506\n",
            "Epoch 362/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.0504\n",
            "Epoch 363/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.0503\n",
            "Epoch 364/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.0492\n",
            "Epoch 365/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0491\n",
            "Epoch 366/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.0494\n",
            "Epoch 367/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.0490\n",
            "Epoch 368/500\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.0487\n",
            "Epoch 369/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.0483\n",
            "Epoch 370/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.0476\n",
            "Epoch 371/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.0469\n",
            "Epoch 372/500\n",
            "5/5 [==============================] - 3s 530ms/step - loss: 0.0467\n",
            "Epoch 373/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0474\n",
            "Epoch 374/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.0466\n",
            "Epoch 375/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.0468\n",
            "Epoch 376/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.0462\n",
            "Epoch 377/500\n",
            "5/5 [==============================] - 3s 494ms/step - loss: 0.0456\n",
            "Epoch 378/500\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.0456\n",
            "Epoch 379/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0449\n",
            "Epoch 380/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.0448\n",
            "Epoch 381/500\n",
            "5/5 [==============================] - 3s 524ms/step - loss: 0.0444\n",
            "Epoch 382/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0442\n",
            "Epoch 383/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0440\n",
            "Epoch 384/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.0440\n",
            "Epoch 385/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.0439\n",
            "Epoch 386/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.0434\n",
            "Epoch 387/500\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.0431\n",
            "Epoch 388/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.0434\n",
            "Epoch 389/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0429\n",
            "Epoch 390/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.0426\n",
            "Epoch 391/500\n",
            "5/5 [==============================] - 2s 489ms/step - loss: 0.0420\n",
            "Epoch 392/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.0417\n",
            "Epoch 393/500\n",
            "5/5 [==============================] - 2s 490ms/step - loss: 0.0413\n",
            "Epoch 394/500\n",
            "5/5 [==============================] - 3s 496ms/step - loss: 0.0416\n",
            "Epoch 395/500\n",
            "5/5 [==============================] - 2s 492ms/step - loss: 0.0414\n",
            "Epoch 396/500\n",
            "5/5 [==============================] - 2s 493ms/step - loss: 0.0408\n",
            "Epoch 397/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.0403\n",
            "Epoch 398/500\n",
            "5/5 [==============================] - 3s 495ms/step - loss: 0.0402\n",
            "Epoch 399/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.0397\n",
            "Epoch 400/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0395\n",
            "Epoch 401/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.0402\n",
            "Epoch 402/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.0392\n",
            "Epoch 403/500\n",
            "5/5 [==============================] - 3s 499ms/step - loss: 0.0390\n",
            "Epoch 404/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0394\n",
            "Epoch 405/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.0391\n",
            "Epoch 406/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0387\n",
            "Epoch 407/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0385\n",
            "Epoch 408/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.0380\n",
            "Epoch 409/500\n",
            "5/5 [==============================] - 3s 525ms/step - loss: 0.0376\n",
            "Epoch 410/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.0378\n",
            "Epoch 411/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.0375\n",
            "Epoch 412/500\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.0378\n",
            "Epoch 413/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.0367\n",
            "Epoch 414/500\n",
            "5/5 [==============================] - 3s 517ms/step - loss: 0.0371\n",
            "Epoch 415/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.0369\n",
            "Epoch 416/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0373\n",
            "Epoch 417/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0366\n",
            "Epoch 418/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.0365\n",
            "Epoch 419/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.0365\n",
            "Epoch 420/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0359\n",
            "Epoch 421/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.0359\n",
            "Epoch 422/500\n",
            "5/5 [==============================] - 3s 510ms/step - loss: 0.0354\n",
            "Epoch 423/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.0357\n",
            "Epoch 424/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.0355\n",
            "Epoch 425/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.0356\n",
            "Epoch 426/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.0348\n",
            "Epoch 427/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.0348\n",
            "Epoch 428/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0351\n",
            "Epoch 429/500\n",
            "5/5 [==============================] - 3s 495ms/step - loss: 0.0349\n",
            "Epoch 430/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.0346\n",
            "Epoch 431/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.0339\n",
            "Epoch 432/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0338\n",
            "Epoch 433/500\n",
            "5/5 [==============================] - 2s 494ms/step - loss: 0.0341\n",
            "Epoch 434/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.0336\n",
            "Epoch 435/500\n",
            "5/5 [==============================] - 2s 502ms/step - loss: 0.0337\n",
            "Epoch 436/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0332\n",
            "Epoch 437/500\n",
            "5/5 [==============================] - 3s 509ms/step - loss: 0.0332\n",
            "Epoch 438/500\n",
            "5/5 [==============================] - 3s 501ms/step - loss: 0.0328\n",
            "Epoch 439/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.0327\n",
            "Epoch 440/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.0327\n",
            "Epoch 441/500\n",
            "5/5 [==============================] - 3s 540ms/step - loss: 0.0322\n",
            "Epoch 442/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0322\n",
            "Epoch 443/500\n",
            "5/5 [==============================] - 3s 504ms/step - loss: 0.0326\n",
            "Epoch 444/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.0323\n",
            "Epoch 445/500\n",
            "5/5 [==============================] - 2s 500ms/step - loss: 0.0314\n",
            "Epoch 446/500\n",
            "5/5 [==============================] - 2s 500ms/step - loss: 0.0319\n",
            "Epoch 447/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0314\n",
            "Epoch 448/500\n",
            "5/5 [==============================] - 3s 520ms/step - loss: 0.0315\n",
            "Epoch 449/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0315\n",
            "Epoch 450/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.0312\n",
            "Epoch 451/500\n",
            "5/5 [==============================] - 2s 496ms/step - loss: 0.0307\n",
            "Epoch 452/500\n",
            "5/5 [==============================] - 3s 515ms/step - loss: 0.0309\n",
            "Epoch 453/500\n",
            "5/5 [==============================] - 3s 535ms/step - loss: 0.0307\n",
            "Epoch 454/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.0302\n",
            "Epoch 455/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.0307\n",
            "Epoch 456/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0301\n",
            "Epoch 457/500\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.0298\n",
            "Epoch 458/500\n",
            "5/5 [==============================] - 2s 483ms/step - loss: 0.0300\n",
            "Epoch 459/500\n",
            "5/5 [==============================] - 2s 502ms/step - loss: 0.0295\n",
            "Epoch 460/500\n",
            "5/5 [==============================] - 3s 516ms/step - loss: 0.0302\n",
            "Epoch 461/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.0297\n",
            "Epoch 462/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0294\n",
            "Epoch 463/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.0296\n",
            "Epoch 464/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.0291\n",
            "Epoch 465/500\n",
            "5/5 [==============================] - 3s 528ms/step - loss: 0.0287\n",
            "Epoch 466/500\n",
            "5/5 [==============================] - 2s 495ms/step - loss: 0.0293\n",
            "Epoch 467/500\n",
            "5/5 [==============================] - 2s 500ms/step - loss: 0.0287\n",
            "Epoch 468/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.0287\n",
            "Epoch 469/500\n",
            "5/5 [==============================] - 3s 518ms/step - loss: 0.0290\n",
            "Epoch 470/500\n",
            "5/5 [==============================] - 3s 512ms/step - loss: 0.0286\n",
            "Epoch 471/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0285\n",
            "Epoch 472/500\n",
            "5/5 [==============================] - 3s 506ms/step - loss: 0.0285\n",
            "Epoch 473/500\n",
            "5/5 [==============================] - 3s 522ms/step - loss: 0.0282\n",
            "Epoch 474/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.0282\n",
            "Epoch 475/500\n",
            "5/5 [==============================] - 3s 524ms/step - loss: 0.0279\n",
            "Epoch 476/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0280\n",
            "Epoch 477/500\n",
            "5/5 [==============================] - 3s 496ms/step - loss: 0.0281\n",
            "Epoch 478/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0277\n",
            "Epoch 479/500\n",
            "5/5 [==============================] - 3s 502ms/step - loss: 0.0276\n",
            "Epoch 480/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0275\n",
            "Epoch 481/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0276\n",
            "Epoch 482/500\n",
            "5/5 [==============================] - 3s 493ms/step - loss: 0.0277\n",
            "Epoch 483/500\n",
            "5/5 [==============================] - 2s 501ms/step - loss: 0.0270\n",
            "Epoch 484/500\n",
            "5/5 [==============================] - 3s 488ms/step - loss: 0.0270\n",
            "Epoch 485/500\n",
            "5/5 [==============================] - 3s 503ms/step - loss: 0.0266\n",
            "Epoch 486/500\n",
            "5/5 [==============================] - 2s 498ms/step - loss: 0.0264\n",
            "Epoch 487/500\n",
            "5/5 [==============================] - 2s 497ms/step - loss: 0.0266\n",
            "Epoch 488/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0263\n",
            "Epoch 489/500\n",
            "5/5 [==============================] - 3s 521ms/step - loss: 0.0264\n",
            "Epoch 490/500\n",
            "5/5 [==============================] - 3s 508ms/step - loss: 0.0264\n",
            "Epoch 491/500\n",
            "5/5 [==============================] - 3s 505ms/step - loss: 0.0265\n",
            "Epoch 492/500\n",
            "5/5 [==============================] - 3s 500ms/step - loss: 0.0258\n",
            "Epoch 493/500\n",
            "5/5 [==============================] - 3s 523ms/step - loss: 0.0261\n",
            "Epoch 494/500\n",
            "5/5 [==============================] - 3s 519ms/step - loss: 0.0262\n",
            "Epoch 495/500\n",
            "5/5 [==============================] - 3s 523ms/step - loss: 0.0257\n",
            "Epoch 496/500\n",
            "5/5 [==============================] - 3s 526ms/step - loss: 0.0256\n",
            "Epoch 497/500\n",
            "5/5 [==============================] - 3s 514ms/step - loss: 0.0257\n",
            "Epoch 498/500\n",
            "5/5 [==============================] - 3s 507ms/step - loss: 0.0259\n",
            "Epoch 499/500\n",
            "5/5 [==============================] - 3s 511ms/step - loss: 0.0257\n",
            "Epoch 500/500\n",
            "5/5 [==============================] - 3s 513ms/step - loss: 0.0255\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4b167f5ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok==4.1.1\n",
        "!pip install flask_ngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5_wTj6Au0zY",
        "outputId": "6f589d0c-813d-4860-9312-c8d0b1fff1e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyngrok==4.1.1\n",
            "  Downloading pyngrok-4.1.1.tar.gz (18 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (0.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyngrok==4.1.1) (6.0)\n",
            "Building wheels for collected packages: pyngrok\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-4.1.1-py3-none-any.whl size=15983 sha256=09ced4310c3ad388eb7253d8cf7fc25f2d5b6ff9f877757b9512e2c26373869d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/d9/12/045a042fee3127dc40ba6f5df2798aa2df38c414bf533ca765\n",
            "Successfully built pyngrok\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-4.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting flask_ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.7/dist-packages (from flask_ngrok) (1.1.4)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (1.0.1)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from Flask>=0.8->flask_ngrok) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=0.8->flask_ngrok) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->flask_ngrok) (2022.6.15)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken \"Enter your Ngrok key\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTVpvxfvzwRW",
        "outputId": "dbf05a1b-a2b1-48c0-bd78-cbbe66fd987e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_inference_models():\n",
        "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "    decoder_state_input_h = tf.keras.layers.Input(shape=(256,))\n",
        "    decoder_state_input_c = tf.keras.layers.Input(shape=(256,))\n",
        "\n",
        "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "        decoder_embedding, initial_state=decoder_states_inputs)\n",
        "    decoder_states = [state_h, state_c]\n",
        "    decoder_outputs = decoder_dense(decoder_outputs)\n",
        "    decoder_model = tf.keras.models.Model(\n",
        "        [decoder_inputs] + decoder_states_inputs,\n",
        "        [decoder_outputs] + decoder_states)\n",
        "\n",
        "    return encoder_model, decoder_model"
      ],
      "metadata": {
        "id": "Tdgq4ApCz0Cx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def str_to_tokens( sentence : str ):\n",
        "    words = sentence.lower().split()\n",
        "    tokens_list = list()\n",
        "    for word in words:\n",
        "        tokens_list.append( input_word_dict[ word ] )\n",
        "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=max_input_length , padding='post')\n",
        "\n",
        "\n",
        "enc_model , dec_model = make_inference_models()"
      ],
      "metadata": {
        "id": "eZ9dJanOz3w2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask,render_template,request\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask\n",
        "app=Flask(__name__)\n",
        "\n",
        "run_with_ngrok(app)\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return render_template('index.html')\n",
        "\n",
        "@app.route('/predict',methods=['POST'])\n",
        "def predict_placement():\n",
        "    text = str(request.form.get('uname'))\n",
        "    print(text)\n",
        "    states_values = enc_model.predict(str_to_tokens(text))\n",
        "    empty_target_seq = np.zeros((1, 1))\n",
        "    empty_target_seq[0, 0] = output_word_dict['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition:\n",
        "        dec_outputs, h, c = dec_model.predict([empty_target_seq] + states_values)\n",
        "        sampled_word_index = np.argmax(dec_outputs[0, -1, :])\n",
        "        sampled_word = None\n",
        "        for word, index in output_word_dict.items():\n",
        "            if sampled_word_index == index:\n",
        "                decoded_translation += ' {}'.format(word)\n",
        "                sampled_word = word\n",
        "\n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_output_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        empty_target_seq = np.zeros((1, 1))\n",
        "        empty_target_seq[0, 0] = sampled_word_index\n",
        "        states_values = [h, c]\n",
        "\n",
        "    result= decoded_translation.replace(' end', '')\n",
        "    return render_template('index.html',result=\"Bot: {}\".format(result))\n",
        "app.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6mlFGe1z5xd",
        "outputId": "613a671e-f778-4a7b-90f9-b311108f7673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug: * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://8de1-34-126-130-158.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [02/Oct/2022 17:51:32] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [02/Oct/2022 17:51:33] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is ai\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [02/Oct/2022 17:51:37] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U5InbWb6z8-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}